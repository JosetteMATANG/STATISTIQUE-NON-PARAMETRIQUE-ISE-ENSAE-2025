subtitle = paste0("Bande passante (h) selon Silverman = ", round(h, 3)),
x = "Valeurs",
y = "Densité"
) +
theme_minimal()
# df <- data.frame(x = grid, dens = dens_tri)
ggplot() +
# Histogramme normalisé (les hauteurs correspondent a une densité)
geom_histogram(aes(x = estim_notes, y = after_stat(density)),
bins = 30, fill = "red", color = "yellow", alpha = 0.6) +
# Courbe de densité estimée
geom_line(data = df, aes(x = x, y = dens),
color = "blue", size = 1) +
labs(
title = "Estimation de densité avec noyau de Triweight et histogramme",
subtitle = paste("h =", round(h_silverman, 3)),
x = "Valeurs",
y = "Densité"
) +
theme_minimal()
h_values <- c(0.1, h_silverman / 2, h_silverman, 2 * h_silverman)
# Pour chaque valeur de h on créer un data frame puis on les merge tous.
df_h <- do.call(rbind, lapply(h_values, function(h) {
dens <- sapply(grid, function(x0) density_triweight(x0, X = estim_notes, h = h))
data.frame(
x    = grid,
dens = dens,
h    = paste0('h = ', round(h, 3))# arrondir a 3 chiffres
)
}))
ggplot(df_h, aes(x = x, y = dens, color = h)) +
geom_line(size = 1) +
labs(
title = "  l'estimation de densité avec differents valeurs de h",
x = "Valeurs", y = "Densité"
) +
theme_light()
dens_tri <- sapply(grid, function(x0) density_triweight(x0, X = estim_notes, h = h_amise))
R_K <- 350 / 429
mu2_K <- 1 / 9
# Taille de l'échantillon et écart-type
n <- length(estim_notes)
sd_X <- sd(estim_notes)
# Approximation de R(f'') sous hypothèse normale
Rf2 <- 3 / (8 * sqrt(pi) * sd_X^5)
h_amise <- (R_K / (mu2_K^2 * Rf2 * n))^(1/5)
print(h_amise)
dens_tri <- sapply(grid, function(x0) density_triweight(x0, X = estim_notes, h = h_amise))
df <- data.frame(x = grid, dens = dens_tri)
ggplot() +
geom_histogram(aes(x = estim_notes, y = after_stat(density)),
bins = 30, fill = "red", color = "yellow", alpha = 0.6) +
geom_line(data = df, aes(x = x, y = dens),
color = "blue", size = 1) +
labs(
title = "Estimation de densité avec noyau Triweight",
subtitle = paste("h optimisé par AMISE .h =", round(h_amise, 3)),
x = "Valeurs", y = "Densité"
) +
theme_minimal()
R_K <- 350 / 429
mu2_K <- 1 / 9
# Taille de l'échantillon et écart-type
n <- length(estim_notes)
sd_X <- sd(estim_notes)
# Approximation de R(f'') sous hypothèse normale
Rf2 <- 3 / (8 * sqrt(pi) * sd_X^5)
h_amise <- (R_K / (mu2_K^2 * Rf2 * n))^(1/5)
print(h_amise)
dens_tri <- sapply(grid, function(x0) density_triweight(x0, X = estim_notes, h = h_amise))
df <- data.frame(x = grid, dens = dens_tri)
ggplot() +
geom_histogram(aes(x = estim_notes, y = after_stat(density)),
bins = 30, fill = "red", color = "yellow", alpha = 0.6) +
geom_line(data = df, aes(x = x, y = dens),
color = "blue", size = 1) +
labs(
title = "Estimation de densité avec noyau Triweight",
subtitle = paste("h optimisé par AMISE .h =", round(h_amise, 3)),
x = "Valeurs", y = "Densité"
) +
theme_minimal()
R_K <- 350 / 429
mu2_K <- 1 / 9
# Taille de l'échantillon et écart-type
n <- length(estim_notes)
sd_X <- sd(estim_notes)
# Approximation de R(f'') sous hypothèse normale
Rf2 <- 3 / (8 * sqrt(pi) * sd_X^5)
h_amise <- (R_K / (mu2_K^2 * Rf2 * n))^(1/5)
print(h_amise)
# Fonction de cross-validation
cv_log_likelihood <- function(h, X) {
n <- length(X)
log_dens <- sapply(1:n, function(i) {
xi <- X[i]
x_others <- X[-i]
u <- (xi - x_others) / h
k_vals <- triweight_kernel(u)
f_hat <- mean(k_vals) / h
log(f_hat)
})
-mean(log_dens) # Le signe - car en pratique on prend l'oppose de CV ( On minimise)
}
# Données
X <- c(1.1, 2.3, 1.7, 2.8, 3.2, 1.9, 2.5, 3.7, 2.9, 2.1)
# Taille de l'échantillon
n <- length(X)
# Définition des bornes de l'intervalle
a <- min(X)
b <- max(X)
# Nombre de classes
J <- 4
# Largeur des classes
h <- (b - a) / J
# Définition des bornes des classes
breaks <- seq(a, b, by = h)
# Tracé de l’histogramme
hist_res <- hist(X, breaks = breaks, freq = FALSE, col = "skyblue",
main = "Estimateur par histogramme",
xlab = "X", ylab = "Densité estimée", border = "darkblue")
# Coordonnées du polygone des fréquences :
# abscisses = milieux des classes
mids <- hist_res$mids
# ordonnées = densité estimée (f_hat)
densites <- hist_res$density
# Tracé de la ligne du polygone des fréquences
lines(mids, densites, type = "b", col = "red", lwd = 2, pch = 19)
# Données
X <- c(1.1, 2.3, 1.7, 2.8, 3.2, 1.9, 2.5, 3.7, 2.9, 2.1)
n <- length(X)
mu <- mean(X)
sigma <- sd(X)
# Paramètres de l'histogramme
J <- 4
h <- (max(X) - min(X)) / J
# Données
X <- c(1.1, 2.3, 1.7, 2.8, 3.2, 1.9, 2.5, 3.7, 2.9, 2.1)
n <- length(X)
mu <- mean(X)
sigma <- sd(X)
# Paramètres de l’histogramme
J <- 4
h <- (max(X) - min(X)) / J
# Estimation empirique du MISE par simulation
B <- 500  # nombre de répétitions Monte Carlo
grid_x <- seq(min(X)-1, max(X)+1, length.out = 500)
true_density <- dnorm(grid_x, mean = mu, sd = sigma)
mise_vals <- numeric(B)
for (b in 1:B) {
# Génération d'un nouvel échantillon
sample_b <- rnorm(n, mean = mu, sd = sigma)
breaks_b <- seq(min(sample_b), max(sample_b), length.out = J + 1)
hist_b <- hist(sample_b, breaks = breaks_b, plot = FALSE)
mids_b <- hist_b$mids
f_hat_b <- hist_b$density
# Interpolation de la densité estimée
f_interp <- approx(x = mids_b, y = f_hat_b, xout = grid_x,
method = "constant", rule = 2)$y
# Calcul de l’erreur quadratique intégrée
mise_vals[b] <- mean((f_interp - true_density)^2)
}
# Moyenne des erreurs → estimation du MISE
MISE_estimee <- mean(mise_vals)
# Calcul théorique de l’AMISE
# Pour la densité normale N(mu, sigma^2), on connaît :
I_fprime2 <- 1 / (2 * pi * sigma^5)  # ∫ (f')^2 dx
AMISE <- (1 / (n * h)) + (h^2 / 12) * I_fprime2
# Affichage des résultats
cat("Estimation empirique du MISE :", round(MISE_estimee, 6), "\n")
cat("Valeur théorique de l’AMISE   :", round(AMISE, 6), "\n")
X <- c(1.1, 2.3, 1.7, 2.8, 3.2, 1.9, 2.5, 3.7, 2.9, 2.1)
n <- length(X)
# Choix automatique du nombre de classes selon Sturges
J <- ceiling(log2(n) + 1)
# Bornes des classes
breaks <- seq(min(X), max(X), length.out = J + 1)
# Histogramme sans affichage
hist_res <- hist(X, breaks = breaks, plot = FALSE)
# Effectifs
n_j <- hist_res$counts
# Largeur classes (elles sont égales ici)
h <- diff(breaks)[1]
# Estimation densité
f_hat <- n_j / (n * h)
# Tableau résultats
data.frame(
Classe = paste0("[", round(breaks[-length(breaks)], 2), ", ", round(breaks[-1], 2), "]"),
Effectif = n_j,
Densite_estimee = round(f_hat, 3)
)
# Données
X <- c(1.1, 2.3, 1.7, 2.8, 3.2, 1.9, 2.5, 3.7, 2.9, 2.1)
# Taille de l'échantillon
n <- length(X)
# Définition des bornes de l'intervalle
a <- min(X)
b <- max(X)
# Nombre de classes
#J <- 4
# Largeur des classes
h <- 0.9973
# Définition des bornes des classes
breaks <- seq(a, b, by = h)
X <- c(1.1, 2.3, 1.7, 2.8, 3.2, 1.9, 2.5, 3.7, 2.9, 2.1)
n <- length(X)
# Choix automatique du nombre de classes selon Sturges
J <- ceiling(log2(n) + 1)
# Bornes des classes
breaks <- seq(min(X), max(X), length.out = J + 1)
# Histogramme sans affichage
hist_res <- hist(X, breaks = breaks, plot = FALSE)
# Effectifs
n_j <- hist_res$counts
# Largeur classes (elles sont égales ici)
h <- diff(breaks)[1]
# Estimation densité
f_hat <- n_j / (n * h)
# Tableau résultats
data.frame(
Classe = paste0("[", round(breaks[-length(breaks)], 2), ", ", round(breaks[-1], 2), "]"),
Effectif = n_j,
Densite_estimee = round(f_hat, 3)
)
# Données
X <- c(1.1, 2.3, 1.7, 2.8, 3.2, 1.9, 2.5, 3.7, 2.9, 2.1)
# Taille de l'échantillon
n <- length(X)
# Définition des bornes de l'intervalle
a <- min(X)
b <- max(X)
# Nombre de classes
#J <- 4
# Largeur des classes
h <- 0.9973
# Définition des bornes des classes
breaks <- seq(a, b, by = h)
# Construction de l’histogramme
hist_res <- hist(X, plot = FALSE)
# Effectifs par classe (n_j)
n_j <- hist_res$counts
# Estimateur de la densité par classe
f_hat <- n_j / (n * h)
# Affichage des résultats
data.frame(
Classe = paste0("[", round(breaks[-length(breaks)], 2), ", ", round(breaks[-1], 2), "]"),
Effectif = n_j,
Densite_estimee = round(f_hat, 3)
)
K <- function(x)
{
K <- 0
if (-1 <= x && x <= 1) {
K <- (pi/4)*cos(x*pi/2)
}
return (K)
}
fn <- function(data,x,nb_cla){
n <- length(data)
h <- (max(data)-min(data))/nb_cla
fn <- 0
for (i in 1:n){
fn <- fn + K((x-data[i])/h)
}
fn <- fn/(n*h)
return (fn)
}
library(haven)
ehcvm_welfare_sen2018 <- read_dta("../Larry_Rachid/ehcvm_welfare_sen2018.dta")
# Créer l'histogramme de base
hist(data, breaks = 200, freq = FALSE,
main = "Histogramme des dépenses de consommation par tête (pcexp)",
xlab = "Dépense", col = "lightgray")
epanechnikov_kernel <- function(u) {
k <- 0.75 * (1 - u^2) * (abs(u) <= 1)
return(k)
}
epanechnikov_density <- function(x, data, h) {
n <- length(data)
u <- outer(x, data, function(xi, xj) (xi - xj) / h)
k_values <- epanechnikov_kernel(u)
density <- rowMeans(k_values) / h
return(density);
return(u)
}
h_optimal <- function(data) {
n <- length(data)
sigma <- sd(data)
iqr <- IQR(data)
s <- min(sigma, iqr / 1.34)
h <- 0.9 * s * n^(-1/5)
return(h)
}
data <- c(2.1, 2.3, 1.9, 2.5, 1.7, 2.8, 1.1, 3.2, 3.9, 2.9)
# Estimation de la densité
dens <- density(data)
# Tracé de la densité
plot(dens, main = "Estimation de la densité", xlab = "Valeurs", ylab = "Densité", col = "blue", lwd = 2)
# Génération de données simulées
# Points où on évalue la densité
x <- seq(min(data) - 1, max(data) + 1, length.out = 200)
# Largeur de bande
h <- h_optimal(data)
# Calcul de la densité
density_values <- epanechnikov_density(x, data, h)
# Affichage du résultat
plot(x, density_values, type = "l", lwd = 2, col = "blue",
main = "Estimation de densité - noyau d'Épanchnikov",
xlab = "x", ylab = "Densité")
rug(data)  # Ajoute les observations sur l’axe x
(x <- c(8.1, 7.3, 6.4, 5.1, 5.4, 4.3, 3.8, 4.9, 5.2, 6.5, 7.8, 7.6, 9.3))
(lambda <- median(x))
(X_bin <- ifelse(x >= lambda, 'A', 'B'))
df <- data.frame(index = 1:length(x), valeur = x, catégorie = X_bin)
# Représentation graphique avec ligne reliant les points
ggplot(df, aes(x = index, y = valeur, color = catégorie, group = 1)) +
geom_line(color = "gray50", linewidth = 1) +       # Ligne reliant les points
geom_point(size = 4) +                              # Points individuels
geom_hline(yintercept = lambda, linetype = "dashed", color = "black") +  # Ligne seuil
annotate("text", x = length(x)-1, y = lambda + 0.5, label = paste("Seuil =", lambda), hjust = 1) +
labs(title = "Visualisation de la séquence avec seuil (médiane)",
x = "Position dans la séquence",
y = "Valeur") +
scale_color_manual(values = c("blue", "red")) +
theme_minimal()
# Nombre de séquences homogènes
# p : nombre de A
# m : nombre de B
# n : taille de la séquence
p <- sum(X_bin == 'A')
m <- sum(X_bin == 'B')
n <- length(X_bin)
p
m
n
S <- 1 + sum(as.numeric(X_bin[-1] != X_bin[-n]))
S
E_S <- 2 * p * m / n + 1
Var_S <- (2 * p * m * (2 * p * m - n)) / (n^2 * (n - 1))
E_S
Var_S
Z <- (S - E_S) / sqrt(Var_S)
Z
2 * (1 - pnorm(abs(Z)))
#install.packages("randtests")
library(randtests)
install.packages("randtests")
### Chargement des données
X <- c(1.1, 2.3, 1.7, 2.8, 3.2, 1.9, 2.5, 3.7, 2.9, 2.1)
# Fonction noyau Biweight
biweight_kernel <- function(x) {
ifelse(abs(x) < 1, (15/16) * (1 - x^2)^2, 0)
}
# Fonction d'estimation de la densité
density_estimate <- function(x, sample, h) {
n <- length(sample)
sum <- 0
for (i in 1:n) {
sum <- sum + biweight_kernel((x - sample[i]) / h)
}
return(sum / (n * h))
}
# Définition de la fenêtre de lissage
h <- 0.5
# Calcul des densités estimées pour chaque observation
density_estimates <- sapply(X, density_estimate, sample = X, h = h)
# Affichage des densités estimées
density_estimates
hist(density_estimates,
main = "Histogramme des densités estimées",
xlab = "Densité estimée",
col = "lightblue",
border = "black")
# Calcul de la moyenne
mean_density <- mean(density_estimates)
# Calcul de la variance
var_density <- var(density_estimates)
# Affichage des résultats
cat("Moyenne des densités estimées :", mean_density, "\n")
cat("Variance des densités estimées :", var_density, "\n")
# Calcul de la statistique de test
test_statistic <- (mean_density - 1) / sqrt(var_density / length(density_estimates))
# Calcul de la valeur critique pour un test bilatéral à 5%
alpha <- 0.05
critical_value <- qnorm(1 - alpha / 2)
# Affichage de la statistique de test et de la valeur critique
cat("Statistique de test :", test_statistic, "\n")
cat("Valeur critique :", critical_value, "\n")
# Prise de décision
if (abs(test_statistic) > critical_value) {
cat("Nous rejetons l'hypothèse nulle (H0).\n")
} else {
cat("Nous ne rejetons pas l'hypothèse nulle (H0).\n")
}
set.seed(123)
CV_hist=function(x) # définition de la fonction en fonction de l'échantillon x
{
n=length(x)
a=min(x)
b=max(x)        # valeurs minimale et maximale de x, utilisées pour délimiter les bornes de l’histogramme
N=round(n/5); # nombre maximal de classe à tester
b=b+(b-a)/N
m_CV=1
J0=2/(n-1)
J=1:N;
# Boucle pour tester différents nombres de classes
for (m in 2:(N+1)){
h=(b-a)/m
hatp=1:m
A=(1:m)%*%t((1:n)*0+1)
xx=((1:m)*0+1)%*%t((x-a)/h)
hatp=rowSums(((A-1)<=xx)*(xx<A))/n
J[m-1]=2-(n+1)*sum(hatp^2)
remove(hatp)
J[m-1]=J[m-1]/((n-1)*h)
# Choix du nombre optimal de classes
if (J[m-1]<J0) {m_CV=m; J0=J[m-1]}
}
op=par(mfcol=c(1,2),pty="m",omi=c(0,0,0,0))
plot(2:(N+1),J,type='l',lwd=2,col='darkred',main="La courbe de la fonction de validation croisée',,xlab='nb de classes",ylab='CV')
h=(b-a)/m_CV
hatf=1:m_CV
n=length(x)
m=m_CV
# Calcul de l'estimateur de densité optimal
for (j in 1:m_CV){hatf[j]=sum(((j-1)*h<=x-a)*(x-a<j*h))/(n*h)}
xleft=a-h+(1:m)*h
xright=xleft+h
ybottom=(1:m)*0
ytop=hatf
plot(c(a-h/n,xleft,b),c(0,hatf,0),type="n",xlab="Les classes", ylab="Estimateur de densité",main="Histogramme avec le nombre de classes optimal")
rect(xleft, ybottom, xright, ytop, col = "cyan", border = "darkblue", lwd = 1)
par(op)
return(m_CV)
}
n <- 8000
x <- runif(n, 0, 5)
Cv_hist_stur <- function(sample) {
n <- length(sample)
k_optimal <- floor(1 + log2(n))
return(k_optimal)
}
Cv_hist_stur(x)
# méthode simple mais peu adapté à de grands échantillons
Freedman_Diaconis_hist <- function(sample) {
n <- length(sample)
IQR_value <- IQR(sample) # calcul de l'écart interquartile
h <- 2 * IQR_value * n^(-1/3)
range_x <- max(sample) - min(sample) # etendue des données
k_optimal <- floor(range_x / h)
return(k_optimal)
}
# méthode plus robuste
Freedman_Diaconis_hist(x)
# validation croisée
CV_hist(x)
library(randtests)
data=c(8.1,7.3,6.4,5.1,5.4,4.3,3.8,4.9,5.2,6.5,7.8,7.6,9.3)
data
plot(data)
Signe <- function(data){
n=length(data)
total <-0
for (i in 1:(n-1)) {
if (data[i] > data[i+1]) {
total <- total + 1
}
}
return(total)
}
resultat=Signe(data)
S=resultat
n=length(data)
E<- (n-1)/2
E
V <- (n+1)/12
V
Z =(S - E- (1/2))/ sqrt(V)
Z
if (Z>qnorm(0.975, 0,1) | Z < -qnorm(0.975, 0,1)) {
"On rejette l'hypothese nulle : il y a une tendance."
} else {
"On ne peut pas rejetter l'hypothese nulle : pas de tendance ."
}
plot(data)
if (Z< -qnorm(0.95, 0,1) ) {
"On rejette l'hypothese nulle : dépendance négative."
} else {
"On ne peut pas rejetter l'hypothèse nulle"
}
if (Z>qnorm(0.95, 0,1) ) {
"On rejette l'hypothese nulle : dépendance positive."
} else {
"On ne peut pas rejetter l'hypothèse nulle"
}
p_value = 2*(1-pnorm(Z, 0,1,lower.tail = FALSE ))
p_value
lines <- readLines("Fichier_centralise.Rmd")
grep("\\$\\$.*\\\\emph|\\$\\$", lines)
# Estimation de la densité aux points de x
estimate_density_x <- function(x, h) {
n <- length(x)
f_hat <- numeric(n)
for (i in seq_along(x)) {
u <- (x-x[i]) / h
f_hat[i] <- sum(triangular_kernel(u)) / (n * h)
}
return(f_hat)
}
#knitr::include_graphics("C:/Users/ROSSA/Desktop/TP_Stat non para/TableG.png")
#knitr::include_graphics("C:/Users/ROSSA/Desktop/TP_Stat non para/TableG.png")
#knitr::include_graphics("C:/Users/ROSSA/Desktop/TP_Stat non para/TableG.png")
